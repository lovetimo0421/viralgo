import argparse
import re
from pathlib import Path
from openpyxl import load_workbook

ENCODING = "utf-8"

def enc_name(enc: str | None = None):
    e = (enc or ENCODING).lower()
    if e in ("utf8mb4", "utf-8-mb4", "utf8"):
        return "utf-8-sig"
    return e


def strip_hyperlink(val: str) -> str:
    s = "" if val is None else str(val)
    if s.startswith("="):
        m = re.search(r"HYPERLINK\(\s*\"[^\"]*\"\s*,\s*\"([^\"]*)\"\s*\)", s, flags=re.I)
        if m:
            return m.group(1)
    return s


def name_allowed(s: str) -> bool:
    if not s:
        return False
    if any(ord(ch) > 127 for ch in s):
        return False
    return re.fullmatch(r"[A-Za-z0-9 \-_'&:().]+", s) is not None


def process_sheet(ws, top_n: int, year: int):
    rows = list(ws.iter_rows(values_only=True))
    if not rows:
        return []
    header = [str(h) if h is not None else "" for h in rows[0]]
    
    def find_col(name: str):
        for i, h in enumerate(header):
            if name.lower() == str(h).strip().lower():
                return i
        return None
    
    def find_contains(sub: str):
        for i, h in enumerate(header):
            if sub.lower() in str(h).strip().lower():
                return i
        return None

    name_i = find_col("Name")
    genre_i = find_contains("genre")
    
    # Find Peak column: prefer "{year} Peak", then just "{year}", then generic "Peak" (excluding All-Time)
    peak_i = None
    
    # 1. Try exact year match
    candidates = [f"{year} Peak", f"{year}", str(year)]
    for c in candidates:
        p = find_col(c)
        if p is not None:
            peak_i = p
            break
            
    # 2. If not found, look for "Peak" but NOT "All-Time"
    if peak_i is None:
        for i, h in enumerate(header):
            h_str = str(h).strip().lower()
            if "peak" in h_str and "all" not in h_str and "time" not in h_str:
                peak_i = i
                break
    
    # 3. Fallback: If we have columns ending with [..., Year Peak, All-Time Peak], pick second to last
    if peak_i is None and len(header) >= 2:
         # Heuristic: often the layout is Name, ..., YearPeak, AllTimePeak
         # Let's assume the user meant "Second to last column" if we can't find by name
         peak_i = len(header) - 2
         print(f"Warning: Could not find specific peak column for {year}, defaulting to column index {peak_i} ({header[peak_i]})")

    # If we can't find essential columns, try to guess or skip
    if name_i is None:
        # Fallback: maybe column 2 is name?
        if len(header) > 2:
            name_i = 2
    
    out = []
    for r in rows[1:]:
        # Extract Name
        nm = ""
        if name_i is not None and name_i < len(r):
            nm0 = strip_hyperlink(r[name_i])
            nm = "" if nm0 is None else str(nm0).replace(",", "").strip()
        
        if not name_allowed(nm):
            continue

        # Extract Genres
        gn = ""
        if genre_i is not None and genre_i < len(r):
            gv = r[genre_i]
            gn = "" if gv is None else str(gv).strip()
        
        # Extract Peak
        pk = 0
        if peak_i is not None and peak_i < len(r):
            pv = r[peak_i]
            if pv is not None:
                try:
                    # Remove commas and parse
                    pk_str = str(pv).replace(",", "")
                    pk = int(pk_str)
                except ValueError:
                    pk = 0

        # Output format: Name, Genres, Peak
        out.append([nm, gn, pk])
        
        if len(out) >= top_n:
            break
            
    return out


def normalize_genres(data_dir: Path, encoding: str = "utf-8"):
    """
    Reads all CSVs in data_dir, builds a global map of Name -> Union(Genres),
    and rewrites the CSVs with unified genres.
    Assumes CSV format: Name, Genres, Peak (as generated by process_sheet above).
    """
    import csv
    
    print("Normalizing genres across all years...")
    files = sorted(data_dir.glob("*.csv"))
    
    # Pass 1: Build global map
    global_genres = {}
    
    for fp in files:
        try:
            with fp.open("r", encoding=enc_name(encoding), newline="") as f:
                reader = csv.reader(f)
                for row in reader:
                    if not row or len(row) < 2:
                        continue
                    name = row[0].strip()
                    genres_str = row[1].strip()
                    if not name:
                        continue
                    
                    parts = [g.strip() for g in genres_str.split(";") if g.strip()]
                    if name not in global_genres:
                        global_genres[name] = set()
                    global_genres[name].update(parts)
        except Exception as e:
            print(f"Error reading {fp} for normalization: {e}")

    # Pass 2: Rewrite files
    for fp in files:
        rows = []
        changed = False
        try:
            # Read
            with fp.open("r", encoding=enc_name(encoding), newline="") as f:
                reader = csv.reader(f)
                for row in reader:
                    if not row or len(row) < 3:
                        rows.append(row)
                        continue
                    
                    name = row[0].strip()
                    original_genres = row[1].strip()
                    
                    if name in global_genres:
                        # Get unified genres
                        unified = sorted(list(global_genres[name]))
                        new_genres = ";".join(unified)
                        if new_genres != original_genres:
                            row[1] = new_genres
                            changed = True
                    
                    rows.append(row)
            
            # Write back if changed (or just always write to be safe)
            if changed:
                write_csv(rows, fp, encoding)
                print(f"Updated {fp.name}")
                
        except Exception as e:
            print(f"Error updating {fp}: {e}")


def write_csv(rows, out_path: Path, encoding: str | None = None):
    out_path.parent.mkdir(parents=True, exist_ok=True)
    try:
        with out_path.open("w", encoding=enc_name(encoding), newline="") as f:
            for r in rows:
                line = ",".join(["" if v is None else str(v) for v in r])
                f.write(line + "\n")
    except PermissionError:
        stem = out_path.stem
        suffix = out_path.suffix or ".csv"
        i = 1
        while i <= 100:
            cand = out_path.with_name(f"{stem}_{i}{suffix}")
            try:
                with cand.open("w", encoding=enc_name(encoding), newline="") as f:
                    for r in rows:
                        line = ",".join(["" if v is None else str(v) for v in r])
                        f.write(line + "\n")
                break
            except PermissionError:
                i += 1


def write_xlsx(rows, out_path: Path):
    from openpyxl import Workbook
    out_path.parent.mkdir(parents=True, exist_ok=True)
    try:
        wb = Workbook()
        ws = wb.active
        ws.title = out_path.stem
        for r in rows:
            ws.append(["" if v is None else v for v in r])
        wb.save(out_path)
    except PermissionError:
        stem = out_path.stem
        suffix = ".xlsx"
        i = 1
        while i <= 100:
            cand = out_path.with_name(f"{stem}_{i}{suffix}")
            try:
                wb = Workbook()
                ws = wb.active
                ws.title = out_path.stem
                for r in rows:
                    ws.append(["" if v is None else v for v in r])
                wb.save(cand)
                break
            except PermissionError:
                i += 1


def main():
    p = argparse.ArgumentParser()
    p.add_argument("--input", type=str, default=str(Path("Data") / "data4tenYrs.xlsx"))
    p.add_argument("--out-dir", type=str, default=str(Path("Data") / "years"))
    p.add_argument("--top", type=int, default=1000)
    p.add_argument("--encoding", type=str, default="utf-8")
    p.add_argument("--excel", action="store_true")
    args = p.parse_args()

    in_path = Path(args.input)
    out_dir = Path(args.out_dir)
    if not in_path.exists():
        print(f"missing input: {in_path}")
        return
    wb = load_workbook(in_path, read_only=True, data_only=True)
    for sh in wb.sheetnames:
        m = re.search(r"(\d{4})", str(sh))
        if not m:
            continue
        yr = int(m.group(1))
        if yr < 2010 or yr > 2025:
            continue
        ws = wb[sh]
        rows = process_sheet(ws, args.top, yr)
        out_path = out_dir / f"{yr}.csv"
        write_csv(rows, out_path, args.encoding)
        if args.excel:
            out_xlsx = out_dir / f"{yr}.xlsx"
            write_xlsx(rows, out_xlsx)
    
    # After processing all sheets, normalize genres
    normalize_genres(out_dir, args.encoding)


if __name__ == "__main__":
    main()
